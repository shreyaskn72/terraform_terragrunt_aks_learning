**Lab 12 is true production AKS territory** ğŸ§±
This lab connects **AKS + Azure Monitor + real persistent storage**, which is exactly what real workloads need.

Iâ€™ll give you:

* Clear mental model
* Terraform changes (AKS add-ons)
* Kubernetes YAML for PVC
* Validation steps (kubectl + Azure Portal)
* Why each piece exists

---

# ğŸ§  Lab 12 â€“ AKS Storage & Add-ons (Deep Explanation)

## ğŸ¯ Goal (What Youâ€™re Actually Proving)

You want to prove that:

1. AKS can **send logs & metrics to Azure**
2. AKS workloads can **persist data**
3. Storage survives pod restarts

This lab answers:

> â€œIs this AKS cluster production-ready?â€

---

# ğŸ§  Big Picture Architecture

```
AKS
 â”œâ”€â”€ Azure Monitor (addon)
 â”‚    â””â”€â”€ Log Analytics Workspace
 â”‚
 â””â”€â”€ Workload Pod
      â””â”€â”€ PVC
           â””â”€â”€ Azure Disk
```

---

# ğŸ“ Lab 12 Folder Structure

```
lab-12/
â”œâ”€â”€ main.tf
â”œâ”€â”€ variables.tf
â”œâ”€â”€ outputs.tf
â”œâ”€â”€ providers.tf
â””â”€â”€ versions.tf
```

(Weâ€™ll deploy storage using **kubectl YAML**, not Terraform â€” this is best practice.)

---

# ğŸ“„ `versions.tf`

```hcl
terraform {
  required_version = ">= 1.5.0"

  required_providers {
    azurerm = {
      source  = "hashicorp/azurerm"
      version = "~> 3.90"
    }
  }
}
```

---

# ğŸ“„ `providers.tf`

```hcl
provider "azurerm" {
  features {}
}
```

---

# ğŸ“„ `variables.tf`

```hcl
variable "resource_group_name" {
  default = "rg-lab-12"
}

variable "location" {
  default = "East US"
}

variable "cluster_name" {
  default = "aks-lab-12"
}
```

---

# ğŸ“„ `main.tf`

## ğŸ”¹ Resource Group

```hcl
resource "azurerm_resource_group" "this" {
  name     = var.resource_group_name
  location = var.location
}
```

---

## ğŸ”¹ Log Analytics Workspace

```hcl
resource "azurerm_log_analytics_workspace" "this" {
  name                = "law-${var.cluster_name}"
  location            = var.location
  resource_group_name = azurerm_resource_group.this.name
  sku                 = "PerGB2018"
  retention_in_days   = 30
}
```

ğŸ§  Why this exists:

* Azure Monitor **needs** a workspace
* Stores:

  * Container logs
  * Node metrics
  * Kubernetes events

---

## ğŸ”¹ AKS Cluster with Azure Monitor Enabled

```hcl
resource "azurerm_kubernetes_cluster" "this" {
  name                = var.cluster_name
  location            = var.location
  resource_group_name = azurerm_resource_group.this.name
  dns_prefix          = var.cluster_name

  default_node_pool {
    name       = "system"
    node_count = 2
    vm_size    = "Standard_DS2_v2"
  }

  identity {
    type = "SystemAssigned"
  }

  oms_agent {
    log_analytics_workspace_id = azurerm_log_analytics_workspace.this.id
  }
}
```

---

## ğŸ§  DEEP EXPLANATION â€“ Azure Monitor Add-on

### ğŸ”¹ `oms_agent`

This enables:

* Container Insights
* Node CPU / Memory
* Pod logs
* Cluster health

Azure:

* Installs monitoring agents as DaemonSets
* Sends data to Log Analytics
* No manual configuration needed

ğŸ‘‰ This is **mandatory for production AKS**

---

# ğŸ“„ `outputs.tf`

```hcl
output "log_analytics_workspace" {
  value = azurerm_log_analytics_workspace.this.name
}
```

---

# â–¶ï¸ Deploy Infrastructure

```bash
terraform init
terraform apply
```

â³ Wait ~5 minutes

---

# ğŸ” Get AKS Credentials

```bash
az aks get-credentials \
  --resource-group rg-lab-12 \
  --name aks-lab-12
```

---

# ğŸ§  AKS Storage Model (Important)

### Azure Disk:

* Block storage
* Attached to **one node**
* Perfect for:

  * Databases
  * Stateful apps

AKS automatically installs:

* **CSI drivers** for Azure Disk
* Default `StorageClass`

---

# ğŸ“„ Deploy Persistent Storage (PVC)

## ğŸ“„ `pvc.yaml`

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: azure-disk-pvc
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: managed-csi
  resources:
    requests:
      storage: 5Gi
```

Apply it:

```bash
kubectl apply -f pvc.yaml
```

---

## ğŸ§  What Happens Behind the Scenes

1. Kubernetes sees PVC
2. CSI driver provisions **Azure Managed Disk**
3. Disk attaches to a node
4. PVC becomes `Bound`

Check:

```bash
kubectl get pvc
```

Expected:

```text
azure-disk-pvc   Bound   pvc-xxxx   5Gi
```

---

# ğŸ“„ Deploy Test Pod Using PVC

## ğŸ“„ `pod.yaml`

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: disk-test
spec:
  containers:
  - name: app
    image: busybox
    command: ["sh", "-c", "echo hello > /data/hello.txt && sleep 3600"]
    volumeMounts:
    - mountPath: /data
      name: disk
  volumes:
  - name: disk
    persistentVolumeClaim:
      claimName: azure-disk-pvc
```

Apply:

```bash
kubectl apply -f pod.yaml
```

---

# âœ… Validate Persistent Storage

### Exec into pod:

```bash
kubectl exec -it disk-test -- cat /data/hello.txt
```

Output:

```text
hello
```

### Delete pod:

```bash
kubectl delete pod disk-test
```

### Recreate pod:

```bash
kubectl apply -f pod.yaml
```

### Check again:

```bash
kubectl exec -it disk-test -- cat /data/hello.txt
```

ğŸ¯ **Data still exists â†’ persistent storage works**

---

# ğŸ” Validate Azure Monitor (Portal)

1. Azure Portal â†’ AKS â†’ **Insights**
2. View:

   * Nodes
   * Pods
   * Logs
3. Go to **Log Analytics Workspace**
4. Run sample query:

```kusto
ContainerLog
| take 10
```

ğŸ¯ **Logs flowing successfully**

---

# âœ… Deliverables Achieved

âœ” Azure Monitor enabled

âœ” Log Analytics connected

âœ” PVC created

âœ” Azure Disk attached

âœ” Data persisted

âœ” Logs visible in Azure

---

# ğŸ§  What You Learned in Lab 12

You now know:

* How AKS monitoring works
* How CSI storage works
* Difference between ephemeral vs persistent
* Why Azure Disk is production-grade

This is **real-world AKS**.

---
