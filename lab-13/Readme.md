**Lab 13 is where your AKS stops looking like a tutorial cluster and starts looking like something youâ€™d actually run in production** ğŸ’ª
This lab is all about **node pools as a design tool**, not just compute.

Iâ€™ll give you:

* Clear mental model
* Full Terraform solution
* Deep explanation of *why each pool exists*
* How upgrade works
* What to verify

---

# ğŸ§  Lab 13 â€“ Advanced AKS Configuration (Node Pools)

## ğŸ¯ Goal (Translated to Real Life)

You want:

* Separation of **system** and **workload** nodes
* Automatic scaling based on demand
* Cheap, interruptible compute for non-critical workloads
* Ability to upgrade node pools independently

ğŸ¯ **Deliverable**:

> A **multi-node-pool AKS cluster**

---

# ğŸ§  Mental Model (Very Important)

In AKS:

* **Cluster** = control plane (managed by Azure)
* **Node pools** = VM Scale Sets
* Each node pool:

  * Can scale independently
  * Can upgrade independently
  * Can have different VM types
  * Can be spot / regular

---

# ğŸ“ Lab 13 Folder Structure

```
lab-13/
â”œâ”€â”€ main.tf
â”œâ”€â”€ variables.tf
â”œâ”€â”€ outputs.tf
â”œâ”€â”€ providers.tf
â””â”€â”€ versions.tf
```

(Simple on purpose â€” focus is AKS tuning.)

---

# ğŸ“„ `versions.tf`

```hcl
terraform {
  required_version = ">= 1.5.0"

  required_providers {
    azurerm = {
      source  = "hashicorp/azurerm"
      version = "~> 3.90"
    }
  }
}
```

---

# ğŸ“„ `providers.tf`

```hcl
provider "azurerm" {
  features {}
}
```

---

# ğŸ“„ `variables.tf`

```hcl
variable "resource_group_name" {
  default = "rg-lab-13"
}

variable "location" {
  default = "East US"
}

variable "cluster_name" {
  default = "aks-lab-13"
}

variable "kubernetes_version" {
  default = null
}
```

---

# ğŸ“„ `main.tf`

## ğŸ”¹ Resource Group

```hcl
resource "azurerm_resource_group" "this" {
  name     = var.resource_group_name
  location = var.location
}
```

---

## ğŸ”¹ AKS Cluster (System Node Pool Only)

```hcl
resource "azurerm_kubernetes_cluster" "this" {
  name                = var.cluster_name
  location            = var.location
  resource_group_name = azurerm_resource_group.this.name
  dns_prefix          = var.cluster_name

  kubernetes_version = var.kubernetes_version

  default_node_pool {
    name       = "system"
    vm_size    = "Standard_DS2_v2"
    node_count = 2
  }

  identity {
    type = "SystemAssigned"
  }
}
```

---

## ğŸ§  Why Only System Pool Here?

* System pool:

  * Runs CoreDNS
  * Runs kube-proxy
  * Runs Azure agents
* Should be:

  * Stable
  * Not spot
  * Not aggressively autoscaled

---

# ğŸ”¹ User Node Pool (Autoscaling)

```hcl
resource "azurerm_kubernetes_cluster_node_pool" "user" {
  name                  = "userpool"
  kubernetes_cluster_id = azurerm_kubernetes_cluster.this.id
  vm_size               = "Standard_DS2_v2"

  mode = "User"

  enable_auto_scaling = true
  min_count           = 1
  max_count           = 5

  node_labels = {
    workload = "general"
  }
}
```

---

## ğŸ§  Why User Pool + Autoscaling?

* Keeps workloads off system nodes
* Scales based on demand
* Kubernetes scheduler places pods here by default (unless constrained)

---

# ğŸ”¹ Spot Node Pool (Cheap Compute)

```hcl
resource "azurerm_kubernetes_cluster_node_pool" "spot" {
  name                  = "spotpool"
  kubernetes_cluster_id = azurerm_kubernetes_cluster.this.id
  vm_size               = "Standard_DS2_v2"

  mode = "User"

  priority        = "Spot"
  eviction_policy = "Delete"
  spot_max_price  = -1

  enable_auto_scaling = true
  min_count           = 0
  max_count           = 3

  node_labels = {
    workload = "spot"
  }

  node_taints = [
    "kubernetes.azure.com/scalesetpriority=spot:NoSchedule"
  ]
}
```

---

## ğŸ§  Why Spot Pool?

* Uses **unused Azure capacity**
* 70â€“90% cheaper
* Can be evicted anytime

### Why taints?

* Prevents critical workloads from landing here
* Only pods with tolerations can use spot nodes

---

# ğŸ§  Scheduling Example (Conceptual)

```yaml
tolerations:
- key: "kubernetes.azure.com/scalesetpriority"
  operator: "Equal"
  value: "spot"
  effect: "NoSchedule"
```

---

# ğŸ“„ `outputs.tf`

```hcl
output "cluster_name" {
  value = azurerm_kubernetes_cluster.this.name
}
```

---

# â–¶ï¸ Deploy the Cluster

```bash
terraform init
terraform apply
```

â³ ~5â€“10 minutes

---

# ğŸ” Connect to AKS

```bash
az aks get-credentials \
  --resource-group rg-lab-13 \
  --name aks-lab-13
```

---

# âœ… Validate Multi-Pool Cluster

```bash
kubectl get nodes -L workload
```

### Example output:

```text
NAME                                STATUS   ROLES   WORKLOAD
aks-system-xxxxx-vmss000000         Ready    agent
aks-userpool-xxxxx-vmss000000       Ready    agent   general
aks-spotpool-xxxxx-vmss000000       Ready    agent   spot
```

ğŸ¯ **Deliverable achieved**:

> Multi-pool AKS cluster

---

# ğŸ”„ Perform Node Pool Upgrade

## Step 1: Check versions

```bash
az aks get-upgrades \
  --resource-group rg-lab-13 \
  --name aks-lab-13
```

---

## Step 2: Upgrade user pool only

```bash
az aks nodepool upgrade \
  --resource-group rg-lab-13 \
  --cluster-name aks-lab-13 \
  --name userpool \
  --kubernetes-version <new-version>
```

ğŸ§  Why this matters:

* No control plane downtime
* System pool untouched
* Workloads migrate gradually

---

# ğŸ§  What You Learned in Lab 13

You now understand:

* System vs user node pools
* Autoscaling in AKS
* Spot instances
* Taints & tolerations
* Independent node pool upgrades

This is **real production AKS design**.

---

# ğŸ”¥ Common Interview Gold Lines

> â€œWe isolate system workloads, use autoscaling user pools for apps, and spot pools for cost-optimized workloads with taints.â€

That sentence alone sounds **senior-level** ğŸ˜„

---


